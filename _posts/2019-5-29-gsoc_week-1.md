---
title:  "Week 1 (ASR - German)"
date:   2019-5-29
layout: single
author_profile: true
comments: true
tags: [GSoC, ASR]
category: GSoC
toc: true
toc_label: "Contents"
toc_icon: "heart"
---

Link to [Repository of Code](https://github.com/AASHISHAG/asr-german)

### Describe my work briefly

I am excited to start my 12 weeks journey with [Distributed Little Red Hen Labs](http://www.redhenlab.org/). Also, I am incredibly thankful to all my mentors for allowing me to start early for the project, so I can later take a break for a few days during my exams.

### Kaldi
Today, I complete my Week-1. The essential task during this week was to engage deeper in [Kaldi](http://kaldi-asr.org). Kaldi is a toolkit for Speech Recognition written in C++ and licensed under the Apache License v2.0. The documentation is not straight-forward, but I tried to summarize according to my understanding and point out resources that helped on my way. An Automatic Speech Recognition pipeline has four significant steps:

1. Pre-Processing
2. Feature Extraction
3. Acoustic Model
4. Language Model

The following is depicted in the image below:

![](
/others/speech-recognition-pipeline.png)

Additionaly, Kaldi acoustic modeling also supports conventional models (i.e., diagonal Gaussian Mixture Models (GMMs)) and Subspace Gaussian Mixture Models (SGMMs) and thus utilises Phonetic knowledge along with Acoustic Model.

5. Phoneme Model

The following lectures by Dan Povey's, Kaldi's developer, helped me to build my knowledge.

1. [Lecture 1](https://www.danielpovey.com/files/Lecture1.pdf) (Overview of the course; getting started with Kaldi; feature generation)

2. [Lecture 2](https://www.danielpovey.com/files/Lecture2.pdf) (HMMs; monophone training)

3. [Lecture 3](https://www.danielpovey.com/files/Lecture3.pdf) (phonetic-context-dependent model training)

4. [Lecture 4](https://www.danielpovey.com/files/Lecture4.pdf) (basic decoding)

### Dataset

Parallely, an important part of the project is collecting and understanding the requisite German Speech dataset. The datasets have different structure and formats. We planned to use the following open-source corpus:

1. [The Spoken Wikipedia Corpora](https://nats.gitlab.io/swc/)
2. [The M-AILABS Speech Dataset](https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/)
3. [Tuda De - University of Hamburg, Germany](https://www.inf.uni-hamburg.de/)

I will keep you posted about my progress!

### Others

- [My GSoC 2019 Journey](https://aashishag.github.io/categories/#gsoc)
- [Repository of Code](https://github.com/AASHISHAG/asr-german)
